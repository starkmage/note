# Large File Upload

The main issue with large file uploads lies in: **having to upload a large amount of data in a single request, making the entire process lengthy, and requiring a restart from the beginning if it fails**.

Consider this: if we split this request into multiple requests, each request's time would be shortened, and if one request fails, we only need to resend that particular request instead of starting over. Could this solve the large file upload problem?

Based on the above issues, it seems large file uploads need to implement the following requirements:

- Support split upload requests (i.e., chunking)
- Support resumable uploads
- Support displaying upload progress and pausing uploads

## JavaScript File Operations

The FileReader object allows web applications to asynchronously read the contents of files (or raw data buffers) stored on the user's computer, using File or Blob objects to specify the file or data to be read.

The File object can come from a FileList object returned after a user selects files in an `<input>` element, from a DataTransfer object generated by a drag and drop operation, or from the result of executing the mozGetAsFile() method on an HTMLCanvasElement.

All `input` elements with `type="file"` have a `files` property, which returns a `FileList` object through `Element.files`.

A `Blob` object is just a container for binary data and cannot manipulate the binary data itself. The `FileReader` object is specifically designed for manipulating binary data. `FileReader` is mainly **used to read file contents into memory**, and through a series of asynchronous interfaces, local files can be accessed in the main thread.

## File Chunking

In encoded uploads, on the frontend, we just need to first get the binary content of the file, then split its content, and finally upload each chunk to the server.

In JavaScript, the File object is a subclass of the Blob object. The Blob object includes an important method `slice`, through which we can split binary files.

After the server receives these chunks, it can then reassemble them.

However, this approach has some issues:

- Unable to identify which file a chunk belongs to; when multiple requests occur simultaneously, the appended file content may be incorrect
- The chunk upload interface is asynchronous, cannot guarantee that the chunks are concatenated in the order they were requested

## Restoring Chunks

- How to identify that multiple chunks come from the same file? This can be done by passing a `context` parameter for the same file with each chunk request
- How to restore multiple chunks into one file?
  - Confirm all chunks have been uploaded; this can be done by having the client call a `mkfile` interface to notify the server to merge after all chunks are uploaded
  - Find all chunks under the same context, confirm the order of each chunk; this can be done by marking a position index on each chunk
  - Concatenate chunks in order to restore the file

Above, there's an important parameter, the `context`. We need to get a unique identifier for a file, which can be obtained through these two methods:

- Concatenate basic information like filename, file length, etc. To avoid multiple users uploading the same file, additional user information like uid can be concatenated to ensure uniqueness
- Calculate the file's hash based on its binary content; this way, as long as the file content is different, the identifier will be different. The drawback is the large computational cost

## Resumable Upload

Even after splitting large files into chunks for upload, we still need to wait for all chunks to upload. During this wait, various situations might cause some chunks to fail uploading, such as network failures or page closures. Since not all chunks are uploaded, the server cannot be notified to merge the file. This situation can be handled through **resumable upload**.

Resumable upload means: you can continue uploading the unfinished parts from where it was previously uploaded, without having to start from the beginning, saving upload time.

Since the entire upload process is conducted on a per-chunk basis, and the `mkfile` interface is actively called by the client after all chunks are uploaded, implementing resumable upload is quite simple:

- Save information about successfully uploaded chunks after each chunk upload
- When transmitting the same file next time, iterate through the chunk list, only select unuploaded chunks for upload
- After all chunks are uploaded, call the `mkfile` interface to notify the server to merge the file

Therefore, the question becomes how to save information about uploaded chunks. There are generally two strategies for saving:

- Can be saved in the frontend browser through localStorage or similar methods. This method doesn't depend on the server and is relatively easy to implement, but the drawback is that if users clear their local files, the upload records will be lost.
- The server itself knows which chunks have been uploaded, so it can provide an additional interface to query uploaded chunks based on the file context, called before uploading to check the file's historical upload records.

## Upload Progress and Pause

Through the `progress` method in [xhr.upload](https://developer.mozilla.org/en-US/docs/Web/API/XMLHttpRequest/upload), monitoring the upload progress of each chunk can be implemented.

Implementing upload pause is also relatively simple. Through `xhr.abort`, you can cancel the upload of currently unfinished chunks, achieving the effect of pausing the upload. Resuming upload is similar to resumable upload - first get the list of uploaded chunks, then resend the unuploaded chunks.

## Reference Articles

[Frontend Large File Upload](https://juejin.cn/post/6844903860327186445#heading-0)

[JS-Frontend Using Blob and File to Read Files](https://blog.csdn.net/zhq2005095/article/details/89069851)